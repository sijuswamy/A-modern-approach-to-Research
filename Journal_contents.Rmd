---
title: A Study on Impact of Guided Meditation or Music on the Brain Wave Stimulation
author:
  - name: Manju Mathew
    email: manjumathew@icte.ac.in
    affiliation: Some Institute of Technology
    correspondingauthor: true
    footnote: 1
  - name: Bob Security
    email: bob@example.com
    affiliation: Another University
  - name: Cat Memes
    email: cat@example.com
    affiliation: Another University
    footnote: 2
  - name: Derek Zoolander
    email: derek@example.com
    affiliation: Some Institute of Technology
    footnote: 2
address:
  - code: Some Institute of Technology
    address: Department, Street, City, State, Zip
  - code: Another University
    address: Department, Street, City, State, Zip
footnote:
  - code: 1
    text: "This is the first author footnote."
  - code: 2
    text: "Another author footnote."
abstract: |
  There was a long belief that mediation has a significant impact on brain activities. This hypothesis is tested with the study of association of mediation music or guided meditation and brain signals from the auditory cortex and prefortal cortex.

  A speciman analysis of 2 minutes audio signal is generated in the experimental setup in NIMHANS hospitals Bengaluru. The statistical analysis revealed that even a polynomial regression could not capture the complete association effectively. But there is a strong association between mediation audio and signals generated in the auditory cortex. A significant impact on the prefortal cortex could not be extablished in the present study.
keywords: 
  - meditation
  - EEG
  - auditory cortex
  - prefortal cortex
  - polynomial regression
  - Approximate Bayesian Condition
  - joint distribution
  - marginal distribution
 
journal: "ICET Journal of Engineering"
date: "`r Sys.Date()`"
classoption: preprint, 3p, authoryear
bibliography: mybibfile.bib
linenumbers: false
numbersections: true
# Use a CSL with `citation_package = "default"`
# csl: https://www.zotero.org/styles/elsevier-harvard
output: 
  rticles::elsevier_article:
    keep_tex: true
    citation_package: natbib
---


# Bibliography styles

Here are two sample references: @Feynman1963118 [@Dirac1953888].

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height = 5, fig.width = 5, fig.align = "center",echo = FALSE, warning = FALSE, message=FALSE)
```


 

## Introduction

EEG stands for Electroencephalogram which are electrical signals that measured the electrical activity of the brain (St et al., 2016). To get the EEG result, electrodes consisting of small metal discs with thin wires are pasted onto scalp. The electrodes detect tiny electrical charges that result from the activity of your brain cells and thus obtained charges are amplified and appear as a graph on a computer screen, or as a recording that may be printed out on paper. The main purpose of EEG is to detect potential problems (encephalitis, hemorrhage, epilepsy,Parkinson’s disease and other) with brain cell communication by painless method (Healthline, 2012).

Here in the data set, we have four input EEG signals, one output signals and time of each signals.

## Objectives of the study

Our objectives are as follows:

1. To visualize the trend and patterns in the EEG signals over time.

2. To assess the fit of the polynomial regression model to the EEG signals and estimate the parameters of the model.

3. To use a simulation-based approach to estimate the posterior distribution of the model parameters, allowing for uncertainty in the model and data.

## Loading Libraries

```{r}
library(ggplot2) # implementation of the grammar of graphics - complex visualizations
library(GGally)  # create a matrix of scatter plots, with histograms or density plots
library(tidymodels)
```

## Additional Libraries

```{r}
library(moments) # calculating and plotting descriptive statistics
library(dplyr) # data manipulation toolkit for working with structured data
library(ggExtra) # additional functionality to ggplot2
#library(readr) # fast and friendly file reading
library(ggpubr) # customizing ggplot2 based publication ready plots

#setting the theme to light 
theme_set(theme_light())
```

## Importing Dataset


```{r}
df <- read.csv("X.csv")
```

## Task 1: Preliminary data analysis

The initial examination of a data set before conducting more advanced and in-depth analysis is called preliminary data analysis. Here we will be looking at the descriptive summary of data set and explore missing values.

```{r}
#summary(df)
```
Here we can see that the mean of time is 60.02 second and output signal is 0.01876 of amplitude. Interestingly the mean of x2 signal is coming in minus.

## Missing values

As the first step of the analysis, presence of missing values is checked and the status was reported as:

```{r}
# Using is.na function to see 
missing_values <- is.na(df)
missvalues <- sum(missing_values)

print(sprintf("Number of missing value in the dataset is %d", missvalues))
```

## Task 1.1: Time Series Plots of Input and Output EEG Signals

Here, we are focusing on creating time series plots of input and output EEG signals. Our concern is to plot 4 input signals into a single graph. So, for that we will work with Base R `ts()` functions. To do so, we need data either in matrix or vector. So here we are going to convert our input data into matrix using as.matrix() function.

```{r}
# Assigning the matrix representation of the variable input signals x into xmat.Then it prints out the matrix representation of the variable "xmat"

# Then the code assigns the column names "X1", "X2", "X3", and "X4" to the columns of the matrix "x" using the "colnames" function.

# It changes the column names of the matrix x to "X1","X2","X3","X4"

xmat <- as.matrix(df[1:4])
#print(xmat)
```

## Creating Visualization

```{r fig.cap="Time series plot of the given input and output signals"}


# Plotting the time series plot with ts() where it takes input as matrix. We then sets the start and end times of the time series to the minimum and maximum values of the "time" variable. It also sets the frequency of the time series to 1. And then it creates a plot of the time series "xmat.ts" with a main title "Time series plot of Input Signals", x-axis label "Time", and y-axis label "Input signal".

# Here we set frequency = 1 as with this we are indicating that the observations in the time series are at the same level of granularity, that is at the same unit of time (second).  

xmat.ts<-ts(df,start = c(min(df$time),max(df$time)),frequency =1)
plot(xmat.ts,main = "Time series plot of Input Signals", xlab = "Time", ylab = "Input signal")
```
Here, we see that in time is plotted in X-axis, input/output signals in Y-axis.

There is no consistent trend (upward or downward) over the entire time span. The series appears to slowly wander up and down have infrequent peaks. We can say the data show random variation.

In input signals time series plot, we do not see any rapid spikes which shows there is neither external causes nor entry errors could have affected the data. There are no obvious outliers. Also, it’s difficult to judge whether the variance is constant or not.

## Distribution for each EEG signals

We an use histogram, density plot, box plot or violin plot. Though most popular graph is histogram, however, here we will also be creating density plot along with histogram using ggplot2.

As density plot are better at determining the distribution shape because they’re not affected by the number of bins like in histograms. In order word we can say density plot is the continuous and smoothed version of the histogram. In the choice of kernel function in a density plot depends on the characteristics of the data and the goals of the analysis.

For histogram we are using bin width instead of bin as it is more appropriate method. Also, histograms can give us a rough idea of the overall shape of the distribution and the general tendency of the data, and they are useful when data is discrete. However, here our data is continuous.

```{r}
# Mode

# In R we do not have direct function to calculate Mode!

# Here table(df$x) function counts the frequency of each unique value in the "x1" column of "df" and we are sorting it with sort function. Now, we select first element of sorted frequency by [1] and using names() we extract the most frequently occurring value, which is the mode value itself. With as.numeric() we convert all this values into numeric. 

# Worth to remember,this code will break if the column "x1 or x2 or any" contains any missing value but we have zero NA values.

mode_x1 <-  as.numeric(names(sort(table(df$x1),decreasing=TRUE)[1]))
mode_x2 <-  as.numeric(names(sort(table(df$x2),decreasing=TRUE)[1]))

mode_y <-  as.numeric(names(sort(table(df$y),decreasing=TRUE)[1]))

```

## Frequency distribution of the output EEG signal with mean, median and mode representation

```{r fig.cap="Frequency distribution of the output EEG signal"}
#histogram with stats y
ggplot(aes(x = y), data = df) + 
  geom_histogram(color = 'black', fill = "grey" ,binwidth = 0.5) + 
  geom_vline(aes(xintercept=median(y),
                 color="median"), linetype="dashed",
             size=1) +
  geom_vline(aes(xintercept=mean(y),
                 color="mean"), linetype="dashed",
             size=1) +
  geom_vline(aes(xintercept=mode_y,
                 color="mode"), linetype="dashed",
             size=1) +
  scale_color_manual(name = "Legend", values = c(median = "blue", mean = "red", mode = "grey"))+
  geom_rug()+ #Show the individual observations with black lines
  xlab("Output Signal") + 
  ylab("Frequency ") 
```

## Frequency distribution of the Input EEG X1 signal with mean, median and mode representation

```{r fig.cap="Frequency distribution of the inpur x1 EEG signal"}
#histogram with stats x1
ggplot(aes(x = x1), data = df) + 
  geom_histogram(color = 'black', fill = "grey", binwidth = 0.5) + 
  geom_vline(aes(xintercept=median(x1),
                 color="median"), linetype="dashed",
             size=1) +
  geom_vline(aes(xintercept=mean(x1),
                 color="mean"), linetype="dashed",
             size=1) +
  geom_vline(aes(xintercept=mode_x1,
                 color="mode"), linetype="dashed",
             size=1) +
  scale_color_manual(name = "statistics", values = c(median = "blue", mean = "red", mode = "black"))+
  geom_rug()+ #Show the individual observations with black lines
  xlab("Input Signal x1") + 
  ylab("Frequency ") 
```


## Frequency distribution of the Input EEG X2 signal with mean, median and mode representation

```{r,fig.cap="Frequency distribution of the inpur x2 EEG signal"}
#histogram with stats x2
ggplot(aes(x = x2), data = df) + 
  geom_histogram(color = 'black', fill = "grey", binwidth = 0.5) + 
  geom_vline(aes(xintercept=median(x2),
                 color="median"), linetype="dashed",
             size=1) +
  geom_vline(aes(xintercept=mean(x2),
                 color="mean"), linetype="dashed",
             size=1) +
  geom_vline(aes(xintercept=mode_x2,
                 color="mode"), linetype="dashed",
             size=1) +
  scale_color_manual(name = "statistics", values = c(median = "blue", mean = "red", mode = "black"))+
  geom_rug()+ #Show the individual observations with black lines
  xlab("Input Signal x2") + 
  ylab("Frequency ") 
```

As mentioned earlier, our goal is to understand the underlying distribution of the EEG signal and compare it across different conditions so a density plot is more appropriate choice.

Density plots can provide a more accurate representation of the underlying distribution and reveal subtle patterns that may not be visible in histograms. Additionally, density plots are less affected by the choice of bin size, which can make it easier to compare the distributions of different variables.They are smoother, which is easier for feeding back into a computer for further calculations. KDE work by passing a strongly sharped peak (kernel function) over each data point on the x-axis @allen1971mean.

Also, on general EEG signals often have a non-normal distribution, and if we suspect this is the case, we may want to use a non-parametric density estimator such as kernel density estimation (KDE) instead of a parametric one (Duong, 2007).

## Spread and concentration of values for input sound signal


```{r fig.cap="Spread and concentration of values for output EEG signal"}
ggplot(df, aes(x = y)) +
  geom_density(color = 3,
               fill = 4,
               alpha = 0.25, #alpha is used to set the transparency of the density plot.
               kernel = "rectangular") +  #he kernel used can also be changed with kernel argument. The possible options are "gaussian" (default), "rectangular"
  geom_rug()+ #Show the individual observations with black lines
  xlab("Output Signal") +
  ylab("Density ")
```

Skewness is defined as the measure of the asymmetry of a probability distribution of a real-valued random variable about its mean (Doane and Seward, 2011). The output signal have long tail points left showing skewed left. Also the output y seems to have one mode skewness. Here, we also see a values being located in the lower tail or left-side of the distribution, far away from the mean and this is refereed as left tail extremes. This sort of extremes might cause regression line to be skewed, leading to over or under-estimation of the relationships between variables (von Hippel, 2005).

## Spread and concentration of values for input x1 EEG signal


```{r fig.cap="Spread and concentration of values for input x1 EEG signal"}
ggplot(df, aes(x = x1)) +
  geom_density(color = 3,
               fill = 4,
               alpha = 0.25, #alpha is used to set the transparency of the density plot.
               kernel = "rectangular") +  #he kernel used can also be changed with kernel argument. The possible options are "gaussian" (default), "rectangular"
  xlab("Input Signal X1") +
  ylab("Density ")
```

## Spread and concentration of values for input x2 EEG signal


```{r "Spread and concentration of values for input x2 EEG signal"}
ggplot(df, aes(x = x2)) +
  geom_density(color = 3,
               fill = 4,
               alpha = 0.25, #alpha is used to set the transparency of the density plot.
               kernel = "rectangular") +  #he kernel used can also be changed with kernel argument. The possible options are "gaussian" (default), "rectangular"
  xlab("Input Signal X2") +
  ylab("Density ")

```

For all input signals we see bell-shaped curve with mean (peak of the is signal) to be around $\theta$. Also, the highest amplitude of signal is around +6 and lowest is -6. The signals appears. to expand from -6 and shrinks from somewhere between 5-6 in positive range. With this we can say that large number of input signals to be around -6 to 5

As the shape of the distribution starts to expand from -4 and shrink from 5, the majority of the input signal appears to lie between -4 and 5. Here, all the input signal’s tail seems to be nearly touching x-axis which says the distribution is almost symmetrical.

## Task 1.3: Correlation and scatter plots (between different input EEG signals and the output EEG) to examine their dependencies

A scatter plot (aka scatter chart, scatter graph) uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. It helps us to see if there are clusters or patterns in the data set. They can also be used to identify outliers and to explore the distribution of the data. When we add a line of best fit to a scatter plot, we can also see the correlation (positive, negative, or zero) between the two variables (Bertini, Correll and Franconeri, 2020).

And a correlation plot, also known as a scatter plot matrix, is a type of scatter plot that shows the relationship between multiple variables. One main reason to use correlation plot is when we have a large number of variables, and we want to see how they are related to each other (Bertini, Correll and Franconeri, 2020). Here, we will be plotting with using pearson correlation coefficient(PCC) also called Pearson Product moment Correlation Coefficient measures the linear relationship between two variables (Rahman and Zhang, 2015).

Both scatter plots and correlation plots can be used to identify if two variables are positively or negatively correlated. A positive correlation means that as one variable increases, the other variable also increases, and a negative correlation means that as one variable increases, the other variable decreases (Taylor, 1990).

In the following scatter plot, to view the correlation, we will add trend line.

A trend line, also known as a line of best fit, is a line that is used to indicate the overall pattern or direction of the data in a scatter plot. The main purpose of adding a trend line to a scatter plot is to visually examine the relationship between the two variables and to make predictions about future observations (Chen, 2019). For example, if a scatter plot shows a positive linear relationship between two variables, a trend line can be used to estimate the value of one variable based on the value of the other variable.

With trend line we will also add a confidence interval (CI).A confidence interval is a range of values that is used to estimate an unknown population parameter. We will be using 95% of CI. A 95% CI for the mean of a population provides a range of plausible values for the true mean of the population (Smith, 2012). If a new sample is taken, on average 95% of such confidence intervals would contain the true population estimate (Bond, 2004).

```{r fig.cap="Relationship between input X1 EEG signal and Output Signal"}
#Scatter plot for x1 with y

ggplot(df, aes(x = x1, y = y,
                 colour = y)) +
  geom_point(show.legend = TRUE) +
  geom_smooth(method=lm, level=0.95)+ #add linear trend line
  scale_color_gradient(low = "#EEC9E5", high = "#7C4D79") +

  xlab("Input Signal X1") + 
  ylab("Output Signal") + 
  stat_cor(p.accuracy = 0.05, r.accuracy = 0.01, method = "pearson" ) 
```


Depending on how tightly the points cluster together, we may be able to discern a clear trend in the data.The closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship. So here, input signal $X_1$ and output signal tends to have stringer correlation. One top of this, we can see on left upper side pearson correlation value R being 0.80 and p value < 0.05. So, 0.87 indicates a strong positive linear relationship between two continuous variables because the values is closer to 1 (Bertini, Correll and Franconeri, 2020).

## Association of x2 and sound signals

```{r "Relationship between input X2 EEG signal and Output Signal"}
#Scatter plot for x2 with y
ggplot(df, aes(x = x2, y = y,
               colour = y)) +
  geom_point(show.legend = TRUE) +
  geom_smooth(method=lm, level=0.95)+ #add linear trend line
  scale_color_gradient(low = "#EEDBBD", high = "#9F3632") +
   xlab("Input Signal X2") + 
  ylab("Output Signal") + 
  stat_cor(p.accuracy = 0.05, r.accuracy = 0.01, method = "pearson" )
```

Here, data points are spread out and have some cluster. This graph has too many outliers. Here, the trend line is about to be straight so we can say that the data has a weak or lower correlation between input signal $X_2$ and output signal. On top of this R = -0.25 suggests that there is a weak positive association between the two variables, meaning that as one variable increases, the other variable is likely to increase, but only to a limited extent (Laerd.com, 2018).

## Visualization scatter plot with density plot and correlation coefficient in single view

```{r fig.cap="Pair-wise analysis of association"}
# Visualizing thr entire data set distribution and correlation coefficient in single view
ggpairs(df) 
```

The variable names are displayed on the outer edges of the matrix. The boxes along the diagonals display the density plot for each variable. The boxes in the lower left corner display the scatter plot between each variable. The boxes in the upper right corner display the pearson correlation coefficient between each variable (Wolfgang $x_2$ and $y$ is -0.253. The star shows how correlated they are.

Overall, the most correlated output parameters with input signal is $x_1$ with coefficient 0.863,  and $x_2$ with -0.253 being least correlated.

## Task 2: Regression – modeling the relationship between EEG signals

Here, we are going to explain the relationship between the input EEG signals and output EEG signals based on the assumption that the relationship can be expressed as polynomial regression model. We are given with 5 different nonlinear polynomial regression models and out of which we need to find the one that is most suitable.

To get such most suitable one, we will be use Akaike information criterion (AIC) and Bayesian information criterion (BIC).

Using least square, estimating model parameter
Here, from above preliminary analysis, we are not sure which is true value(most suitable) of the model. When we have no idea(unknown) about the true value of that distribution, we use concept of estimator (random variable) (Peterka, 1981). In other word, we are using estimator variable to estimate the true value of the EEG data distribution that relates the input and output variables.

Here, the estimator variable is represented by $\theta$ and can take on multiple values such as $\theta_1,\theta_2,\ldots, \theta_{bias}$. Now, the least squares method (LSM) is used to calculate the estimator model parameters for different candidate models of EEG data and the LSM ($\hat \theta$) is used to estimate the true value of the distribution by minimizing the sum of the squared residuals between the predicted and actual values of the output variable (Bjorck, 1990) which is expressed by the following formula:
$$\hat\theta =(X^TX)^{-1}X^Ty$$
where:

- X is the matrix of input variables
- y is the vector of output variables
- T represents the transpose of a matrix or vector
- $\hat\theta$ is the estimated value of the model parameters (Least square).

Now, to calculate the least squares, we first need to format the input data by binding the appropriate columns or values from the EEG data set. With the function cbind(), we bind the input data using following code: 
x_model1 <- cbind(ones, x[,4], x[,1]^2, x[,1]^3, x[,3]^4)

where:

"ones" represents a column of ones (used for the constant term in the model)
($X_1, X_2, X_3^3, X_4^4$) represent additional columns of input data that are being included in the model.
Once the input data is correctly formatted, we can then use the least squares formula as mentioned above and using the built-in solve linear equations function called `solve()`, we find the $\hat\theta$. We use `solve()` because it's more efficient and less error-prone (Schork, n.d.).

## Regression model-1

Model 1: $y = \theta_1x_1^3 + \theta_2x_2^5 +\theta_{bias} + \epsilon$

```{r}
# creating model
ones = matrix(1 , length(df$x1),1) 
x_model_1<-cbind(ones,(df[,"x1"])^3,(df[,"x2"])^5)
```

```{r}
#finding the coefficients
# We are converting thus obtained dataframe x_model_1  into a matrix using the function as.matrix(). The transpose of the matrix is taken using the function t(). The operator %*% is used for matrix multiplication under solve() function.

theta_hat_1 <- solve(t(as.matrix(x_model_1)) %*% as.matrix(x_model_1)) %*% t(as.matrix(x_model_1)) %*% as.matrix(df$y)

# Viewing the value of theta_hat_1
print(theta_hat_1) 
```

## Building second model

$$y=\theta_0+\theta_1x_1^4+\theta_2x_2^2+\epsilon$$

```{r}
# Binding for model 2
x_model_2<-cbind(ones,(df[,"x1"])^4,(df[,"x2"])^2)

# Calculating theta_hat_2
theta_hat_2 <- solve(t(as.matrix(x_model_2)) %*% as.matrix(x_model_2)) %*% t(as.matrix(x_model_2)) %*% as.matrix(df$y)

# Viewing the value of theta_hat_2
print(theta_hat_2) 
```

## Building model-3

$$y=\theta_0+\theta_1x_1^3+\theta_2x_2+\theta_3 x_1+\epsilon$$

```{r}
# Binding for model 3
x_model_3<-cbind(ones,(df[,"x1"])^3,(df[,"x2"]),df[,"x1"])

# Calculating theta_hat_2
theta_hat_3 <- solve(t(as.matrix(x_model_3)) %*% as.matrix(x_model_3)) %*% t(as.matrix(x_model_3)) %*% as.matrix(df$y)

# Viewing the value of theta_hat_2
print(theta_hat_2) 
```

## Regression Model-4

$$y=\theta_0+\theta_1 x_1+\theta_2 x_1^2+\theta_3 x_1^3+\theta_4 x_2^3+\epsilon$$
```{r}
# Binding for model 4
x_model_4<-cbind(ones,(df[,"x1"]),(df[,"x1"])^2,(df[,"x1"])^3,df[,"x2"]^3)

# Calculating theta_hat_2
theta_hat_4 <- solve(t(as.matrix(x_model_4)) %*% as.matrix(x_model_4)) %*% t(as.matrix(x_model_4)) %*% as.matrix(df$y)

# Viewing the value of theta_hat_2
print(theta_hat_4) 
```

## Regression model-5

$$y=\theta_0+\theta_1 x_1^3+\theta_2 x_1^4+\theta_3 x_2+\epsilon$$

```{r}
# Binding for model 5
x_model_5<-cbind(ones,(df[,"x1"])^3,(df[,"x1"])^4,(df[,"x2"]))

# Calculating theta_hat_2
theta_hat_5 <- solve(t(as.matrix(x_model_5)) %*% as.matrix(x_model_5)) %*% t(as.matrix(x_model_5)) %*% as.matrix(df$y)

# Viewing the value of theta_hat_2
print(theta_hat_5) 
```

## Task 2.2: Calculating model residual errors (RSS)

The residual sum of squares (RSS) also termed as sum of squared errors of prediction (SSE) or sum of squared residuals (SSR) is a measure of discrepancy between the data and an estimation model. It is calculated by subtracting average square of the actual values and the estimated values of the dependent variable, based on the model parameters (Allen, 1971).

We usually want to minimize the error thus the smaller the error, the better the estimation power of the regression and better fit of the model. On the other hand, a larger RSS indicates a worse fit of the model to the data (Barone, 2022). It is worth to mention that the RSS is never negative, because the squares of the residuals are always non-negative (Valchanov, 2018).

To calculate RSS, we first have to calculate error of every models 1-5 with the help of ($\hat\theta$) that we calculated in Task 2.1 and RSS is mathematically presented as:

$$RSS=\sum_{i=1}^{n}(y_i -x_i\hat{\theta})^2$$


## Calculating model residual errors (RSS) for model 1

```{r}
# Calculating Y-hat and RSS model 1

# Before that we convert our model and theta hat into matrix
x_model_1 <- as.matrix(x_model_1)
theta_hat_1 <- as.matrix(theta_hat_1)


Y_hat_model_1 <- x_model_1 %*% theta_hat_1



# Calculating RSS using above mentioned formula

RSS_model_1 <- sum((df$y - Y_hat_model_1)^2)

# printing RSS value for model 1
print(sprintf("RSS value of the model 1 is %0.4f", RSS_model_1))

```

## Calculating model residual errors (RSS) for model 2

```{r}
# Calculating Y-hat and RSS model 2

# Before that we convert our model and theta hat into matrix
x_model_2 <- as.matrix(x_model_2)
theta_hat_2 <- as.matrix(theta_hat_2)


Y_hat_model_2 <- x_model_2 %*% theta_hat_2



# Calculating RSS using above mentioned formula

RSS_model_2 <- sum((df$y - Y_hat_model_2)^2)

# printing RSS value for model 2
#print(paste("RSS value of the Model 2 is", RSS_model_2))

print(sprintf("RSS value of the model 2 is %0.4f", RSS_model_2))

```

## Calculating model residual errors (RSS) for model 3

```{r}
# Calculating Y-hat and RSS model 3

# Before that we convert our model and theta hat into matrix
x_model_3 <- as.matrix(x_model_3)
theta_hat_3 <- as.matrix(theta_hat_3)


Y_hat_model_3 <- x_model_3 %*% theta_hat_3



# Calculating RSS using above mentioned formula

RSS_model_3 <- sum((df$y - Y_hat_model_3)^2)

# printing RSS value for model 3

print(sprintf("RSS value of the model 3 is %0.4f", RSS_model_3))
```

## Calculating model residual errors (RSS) for model 4


```{r}
# Calculating Y-hat and RSS model 4

# Before that we convert our model and theta hat into matrix
x_model_4 <- as.matrix(x_model_4)
theta_hat_4 <-  as.matrix(theta_hat_4)


Y_hat_model_4 <- x_model_4 %*% theta_hat_4



# Calculating RSS using above mentioned formula

RSS_model_4 <- sum((df$y - Y_hat_model_4)^2)

# printing RSS value for model 4

print(sprintf("RSS value of the model 4 is %0.4f", RSS_model_4))
```

## Calculating model residual errors (RSS) for model 5

```{r}
# Calculating Y-hat and RSS model 5

# Before that we convert our model and theta hat into matrix
x_model_5 <- as.matrix(x_model_5)
theta_hat_5 <- as.matrix(theta_hat_5)


Y_hat_model_5 <- x_model_5 %*% theta_hat_5



# Calculating RSS using above mentioned formula

RSS_model_5 <- sum((df$y - Y_hat_model_5)^2)

# printing RSS value for model 5

print(sprintf("RSS value of the model 5 is %0.4f", RSS_model_5))
```

Here, the lowest RSS value in the table is 1525.62, which is associated with model 3.

## Calculating log-likelihood functions

Now, our objective is to identify how well the measured value fits the sample data of a provided model when parameters are unknown. To meet our objective we are going to calculate log-likelihood functions for a linear regression model using RSS that we obtained from Task 2.2.

Log-likelihood is a way to measure the goodness of fit for a model (Zach, 2021) and used to simplify the optimization problem and avoid numerical underflow or overflow. Mathematically presented as:


$$ \ln(D|\hat\theta)=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\hat\sigma^2)-\dfrac{1}{2\hat\sigma^2}RSS$$

In this task, our goal is to find the set of parameters that maximizes the probability of the observations. As the nature of the log-likelihood function is that it increases monotonically (non-decreasing) and has no local maxima, it is suitable for identifying how well the measured value fits (Stephanie, 2021). In layman’s terms, monotonically increasing means that as the value of the independent variable (say $x$) increases, so does the value of the function (say $y$), i.e., as $x$ increases, $y$ can only increase and cannot ever decrease.

So, here as the value of the log-likelihood increases, the likelihood of the data given the model parameters also increases. Therefore, finding the maximum of the log-likelihood function is the same as finding the maximum of the likelihood function but if we go with just likelihood then the concave nature of log is missing and we cannot get the one global maxima (Music, 2020).

Therefore using the above formula, we will first calculate variance of model using RSS along with length of the $Y$ signal and then calculate log-likelihood function.

## Calculating log-likelihood functions for model 1

```{r}
# Calculating length of the output signal y with nrow()  as our data are in matrix format and storing it in N

N <- length(df$y)


# Calculating the variance of model 1

Variance_model_1 = RSS_model_1/(N-1)

# Printing variance of model 1

print(sprintf("Variance of model 1 is %0.4f", Variance_model_1))
```

```{r}

# Calculating the log-likelihood of model 1 using model residual error (RSS)

likehood_model_1 <- -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model_1))-(1/(2*Variance_model_1))*RSS_model_1

# Printing log likelihood function of model 1
print(sprintf("Log-likelihood of model 1 is %0.4f", likehood_model_1))
```

Here, absolute value of log-likelihood of model 1 is -6465.4983, which is large and indicates that the model 1 is a poor fit to the data.

Now similar action for all model are performed except counting $N$.

## Calculating log-likelihood functions for model 2

```{r}
# Calculating the variance of model 2

Variance_model_2 = RSS_model_2/(N-1)

# Printing variance of model 2

print(sprintf("Variance of model 2 is %0.4f", Variance_model_2))

```

```{r}
# Calculating the log-likelihood of model 2 using model residual error (RSS)

likehood_model_2 <- -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model_2))-(1/(2*Variance_model_2))*RSS_model_2

# Printing log likelihood function of model 2
print(sprintf("Log-likelihood of model 2 is %0.4f", likehood_model_2))
```

## Calculating log-likelihood functions for model 3

```{r}
# Calculating the variance of model 3

Variance_model_3 = RSS_model_3/(N-1)

# Printing variance of model 3

print(sprintf("Variance of model 3 is %0.4f", Variance_model_3))
```

```{r}
# Calculating the log-likelihood of model 3 using model residual error (RSS)

likehood_model_3 <- -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model_3))-(1/(2*Variance_model_3))*RSS_model_3

# Printing log likelihood function of model 3
print(sprintf("Log-likelihood of model 3 is %0.4f", likehood_model_3))
```

## Calculating log-likelihood functions for model 4

```{r}
# Calculating the variance of model 4

Variance_model_4 = RSS_model_4/(N-1)

# Printing variance of model 4

print(sprintf("Variance of model 4 is %0.4f", Variance_model_4))

```

```{r}
# Calculating the log-likelihood of model 4 using model residual error (RSS)

likehood_model_4 <- -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model_4))-(1/(2*Variance_model_4))*RSS_model_4

# Printing log likelihood function of model 4
print(sprintf("Log-likelihood of model 4 is %0.4f", likehood_model_4))
```

## Calculating log-likelihood functions for model 5

```{r}
# Calculating the variance of model 5

Variance_model_5 = RSS_model_5/(N-1)

# Printing variance of model 5

print(sprintf("Variance of model 5 is %0.4f", Variance_model_5))

```

```{r}
# Calculating the log-likelihood of model 5 using model residual error (RSS)

likehood_model_5 <- -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model_5))-(1/(2*Variance_model_5))*RSS_model_5

# Printing log likelihood function of model 5
print(sprintf("Log-likelihood of model 5 is %0.4f", likehood_model_5))

```

We have found that all log-likelihood value are negative and in these cases, a higher log-likelihood indicates a better fit, so model 3  have the largest log-likelihoods among the models, which indicates that they fit the data best.

## Task 2.4: Calculating Akaike information criterion and Bayesian Information Criteria

Now as we have RSS and log-likelihood value, we need model selection criteria method for which we will work with Akaike information criterion (AIC) and Bayesian Information Criteria (BIC). According t o(Trevor Hastie, Tibshirani and Friedman, 2009), model selection is estimating the performance of various candidate models with the sole objective of choosing the best one.

Both of them can be used to compare different models and choose the best one. They are both based on the likelihood of the model given the data and the number of parameters in the model. However, the main difference between these two model selection method is that AIC gives less penalty to models with more parameters compared to BIC (Brownlee, 2019).

First, we will start computing AIC for each models.

### Task 2.4.1: Calculating AIC for each models

Akaike information criterion (AIC is a statistical method which aims is to determine the model that truly explains the variance in the dependent variable with the fewest number of independent variables (parameters) (Manikantan, 2021). With this, it helps to select a simpler model containing fewer parameters over a complex model - more parameters.

Using the maximum likelihood estimate (we found the value in Task 2.3), the relative information value of the model and the number of parameters in the model are determined. The formula for AIC is expressed as:

$$AIC=2k- 2\ln(\hat L)$$

Where: $k$ is the number of parameters in the model (from Task 2.1) and $\hat L$ is the maximum likelihood of the estimated model (from Task 2.3).

The major goal of applying AIC in this situation is to eliminate the problem of overfitting because AIC penalizes models with more parameters (independent variables) and so balances the trade-off between a model's goodness of fit and complexity (Bevans, 2020).

According to (Zach, 2021b), one thing worth mentioning is that the model fits the data better when the AIC value is lower. The AIC value’s absolute value is not so important; it could be favorable or unfavorable.

Using the above formula of AIC, we will be calculating the value for each model.

### Calculating AIC for model 1

```{r}
# Calculating AIC for model 1. 
# Here we are finding value of K with length() function.

AIC_1 <- 2* length(x_model_1[1,]) - 2 * likehood_model_1
# Printing AIC of model 1
print(sprintf("AIC of model 1 is %0.4f", AIC_1))

```

### Calculating AIC for model 2


```{r}
# Calculating AIC for model 2. 


AIC_2 <- 2* length(x_model_2[1,]) - 2 * likehood_model_2
# Printing AIC of model 2
print(sprintf("AIC of model 2 is %0.4f", AIC_2))
```

### Calculating AIC for model 3

```{r}
# Calculating AIC for model 3. 


AIC_3 <- 2* length(x_model_3[1,]) - 2 * likehood_model_3
# Printing AIC of model 3
print(sprintf("AIC of model 3 is %0.4f", AIC_3))

```

### Calculating AIC for model 4

```{r}
# Calculating AIC for model 4. 


AIC_4 <- 2* length(x_model_4[1,]) - 2 * likehood_model_4

print(sprintf("Length of parameter to be estimated in model 4 is %d", length(x_model_4[1,])))
# Printing AIC of model 4
print(sprintf("AIC of model 4 is %0.4f", AIC_4))

```

### Calculating AIC for model 5

```{r}
# Calculating AIC for model 5. 


AIC_5 <- 2* length(x_model_5[1,]) - 2 * likehood_model_5
# Printing AIC of model 5
print(sprintf("AIC of model 5 is %0.4f", AIC_5))
```

As we recall, AIC values less than zero indicate that the model is over fitting the data, and AIC values greater than zero indicate that the model is under fitting the data. AIC values closer to zero indicate that the model is a better fit to the data and here model 3 with AIC of 5731.5444 is the best fit among the models listed.

## Calculating BIC for each models

Now, we will calculate Bayesian Information Criteria (BIC). As mentioned earlier, BIC is similar to AIC but in BIC will give a greater penalty on models with more parameters (Datacadamia, 2014). Similar to AIC, lower BIC values indicate better model fit. The formula for BIC is expresses as:

$$BIC=k\ln(n)-2\ln(\hat L)$$

### Calculating BIC for model 1

```{r}
# Calculating BIC for model 1 

BIC_1 <- length(x_model_1[1,]) * log(N) - 2 * likehood_model_1


# Printing BIC of model 1
print(sprintf("BIC of model 1 is %0.4f", BIC_1))

```

### Calculating BIC for model 2

```{r}
# Calculating BIC for model 2

BIC_2 <- length(x_model_2[1,]) * log(N) - 2 * likehood_model_2


# Printing BIC of model 2

print(sprintf("BIC of model 2 is %0.4f", BIC_2))

```

### Calculating BIC for model 3

```{r}
# Calculating BIC for model 3 

BIC_3 <- length(x_model_3[1,]) * log(N) - 2 * likehood_model_3


# Printing BIC of model 3

print(sprintf("BIC of model 3 is %0.4f", BIC_3))

```

### Calculating BIC for model 4

```{r}
# Calculating BIC for model 4

BIC_4 <- length(x_model_4[1,]) * log(N) - 2 * likehood_model_4


# Printing BIC of model 4

print(sprintf("BIC of model 4 is %0.4f", BIC_4))
```

### Calculating BIC for model 5

```{r}
# Calculating BIC for model 5

BIC_5 <- length(x_model_5[1,]) * log(N) - 2 * likehood_model_5


# Printing BIC of model 5

print(sprintf("BIC of model 5 is %0.4f", BIC_5))

```
The smallest BIC value corresponds to Model-3.


## Checking distribution of model prediction errors

Now as we have obtained AIC and BIC value, we are interested in viewing error distribution. After all our goal is to pick one that is showing less error. Before going with the decision of graphic the distribution, we need to calculate error of each model. Then, we will. use Q-Q plot (quantile-quantile plot) to visualize and compare two probability distributions.

```{r}
## calculation of error in each model
##  Error of models 1-5  based on calculation form Task 2.2
model_1_error <- df$y - Y_hat_model_1
model_2_error <- df$y - Y_hat_model_2
model_3_error <- df$y - Y_hat_model_3
model_4_error <- df$y - Y_hat_model_4
model_5_error <- df$y - Y_hat_model_5

```
## Visualizing prediction error with Q-Q plot

Now, as we have the model error for each models, we are going to plot Q-Q plot. According to (Clay Ford, 2015), Q-Q plot is formed by plotting two quantile sets against each other. In the case of the same distribution, both sets of quantiles would form a relatively straight line, however in practise this doesnot mean hard and fast rule. In the same plot we are going to add a reference line to called Q-Q line, which is the line of perfect fit for a normal distribution.

The `qqline()` function takes two arguments, the first is the data- model’s prediction errors and the second is the color (col), the width of the line (lw) along with dashed line(lty).

As from the beginning we are assuming the data are independent and identically distributed i.e. Gaussian with zero-mean, so we are using qqnorm() function to plot the Q-Q plot.

```{r fig.cap="Q-Q plot of model1"}
# Q-Q plots of prediction error for model 1

qqnorm(t(model_1_error),col = "grey")
qqline(model_1_error, col = "red", lwd = 1,lty = 2)
```

```{r fig.cap="Q-Q plot of model2"}
# Q-Q plots of prediction error for model 2

qqnorm(t(model_2_error), col= "grey")
qqline(model_2_error, col = "red", lwd = 1,lty = 2)
```

```{r fig.cap="Q-Q plot of model3"}
# Q-Q plots of prediction error for model 2

qqnorm(t(model_2_error),col= "grey")
qqline(model_2_error, col = "red", lwd = 1,lty = 2)

```

```{r fig.cap="Q-Q plot of model3"}
# Q-Q plots of prediction error for model 3

qqnorm(t(model_3_error), col= "grey")
qqline(model_3_error, col = "red", lwd = 1,lty = 2)
```
Q-Q plot for model 3 shows that most of the data doesn’t follows Q-Q line (red color) thus we can say data doesn’t follows a normal distribution and it is skewed.

```{r fig.cap="Q-Q Plot for Model 4"}
# Q-Q plots of prediction error for model 4

qqnorm(t(model_4_error),col= "grey")
qqline(model_4_error, col = "red", lwd = 1, lty = 2)
```

```{r fig.cap="Q-Q Plot for Model 5"}
# Q-Q plots of prediction error for model 5

qqnorm(t(model_5_error), col= "grey")
qqline(model_5_error, col = "red", lwd = 1,lty = 2)
```

Q-Q plot for model 3 shows that most of the data follows Q-Q line (red color) thus we can say data follows a normal distribution.

Here, with the obtained Q-Q plot we simply visually check if a data set follows a theoretical distribution or not. To formally test whether or not a data set follows a particular distribution, we need to go one step further.

## Selecting Best regression model

By completing tasks 2.1 to 2.5, we have gathered all the necessary information to select the best candidate model. By calculating the RSS, log-likelihood function, plotting normal distribution graphs, and comparing the AIC and BIC values, we have all the information to been identify the best model for our data.

As previously mentioned, AIC and BIC are widely used for model selection because they help to minimize the score of error while selecting a model. In both AIC and BIC, the lowest value is most likely to be best fit, so looking at the data,

Here, the best model fit based on AIC and BIC would be model 3 as it has lowest value. To verify the selected model 3 is good candidate, we will look at the Q-Q plot.

Looking at the Q-Q plot, except model 3, all models seems to have non-normal nature, however, looking at the position of Q-Q line, the model 3 seems to be suitable one.

To get more incline to decision of picking up model 3, we would like to plot histogram to show distribution of residuals. 

```{r fig.cap=" Comparison of error distribution of various models"}
# Distribution of prediction error using histogram
par(mfrow = c(3,2))
hist (model_1_error[,1], freq = FALSE, col="blue", las =1)
abline(v = median(model_1_error[,1]), col = "grey", lwd = 5)
abline(v = mean(model_1_error[,1]), col = "purple", lwd = 5)


hist (model_2_error[,1], freq = FALSE, col="green", las =1)
abline(v = median(model_2_error[,1]), col = "grey", lwd = 5)
abline(v = mean(model_2_error[,1]), col = "purple", lwd = 5)
# abline(v = getmode(model_2_error[,1]), col = "red", lwd = 5)

hist (model_3_error[,1], freq = FALSE, col="orange", las =1)
abline(v = median(model_3_error[,1]), col = "grey", lwd = 5)
abline(v = mean(model_3_error[,1]), col = "purple", lwd = 5)


hist (model_4_error[,1], freq = FALSE, col="yellow", las =1)
abline(v = median(model_4_error[,1]), col = "grey", lwd = 5)
abline(v = mean(model_4_error[,1]), col = "purple", lwd = 5)


hist (model_5_error[,1], freq = FALSE, col="pink", las =1)
abline(v = median(model_5_error[,1]), col = "grey", lwd = 5)
abline(v = mean(model_5_error[,1]), col = "purple", lwd = 5)


hist(0, main = "color code")
legend("center", legend = c("median","mean"),
       lwd = 1, col = c("grey", "purple"))
```

Looking at the distribution of each mode, model 2, 5 both seems to have normal distribution.

Further more, we will go one step extra and see if model 2 is suitable than other. In this case, we will consider additional factors to determine the best model. Here, based on interpretability of the model, i.e., a simpler model with fewer parameters is way easier to interpret and understand (De'ath and Fabricius, 2000), so we will see the number of parameter in each models.

Looking at the each length of parameter, the lowest is model 3 with 3 numbers of parameter, but its doesn't follows a normal distribution and it is skewed and the next is model 4 with number of 4 parameters however its AIC and BIC is greater than the model 2.

So as a conclusion of AIC, BIC, Q-Q plot and extra interpretability, we have picked up model 3 as the best fit model which is expressed as:

$$y=\theta_0+\theta_1x_1^4+\theta_2x_2^2+\epsilon$$
## Splitting data for test and train for selected model 2

Now, as we picked up best suitable model 2, now we are interested on training and testing of the data. For this, we will divide data set into 70% training and 30% testing set. We are using 70/30 ratio because it provides a good balance between having enough data (70%) for the model to learn from during training and having quite reasonable (30%) data to test (Breiman and Spector, 1992) (Nguyen et al., 2021).

So, using initial_split () function, we will split data in 70/30 by providing the numbers into prop argument of the function for both y and x signals.


```{r}
# Splitting input signals x

split_x <- initial_time_split(data = df[2:3],prop=.7)


# Training  data for input signals x  are split and thus splitted training set is converted to matrix form ans assigned to the variable x_training_data 

x_training_set <- training(split_x)

x_training_data <- as.matrix(x_training_set)

# Testing data  are split  and thus splitted testing set is converted to matrix form ans assigned to the variable x_testing_data 

x_testing_set <- testing(split_x)

x_testing_data <- as.matrix(x_testing_set)



# Splitting the data of output signals y into train and test

split_y <- initial_time_split(data = df[4],prop=.7)
 


# Training  data for output signals y  are split and thus splitted training set is converted to matrix form ans assigned to the variable y_training_data 

y_training_set <- training(split_y)

y_training_data <- as.matrix(y_training_set)

# Testing data for output signals y are splitted and thus splitted testing set is converted to matrix form ans assigned to the variable y_testing_data 

y_testing_set <- testing(split_y)

y_testing_data <- as.matrix(y_testing_set)
```

### Estimating model parameter for selected model 3 using training dataset

Following the same approach as per Task 2.1, we will estimate model parameter for our selected model 3, which equation is as per:

$$y=\theta_0+\theta_1x_1^4+\theta_2x_2^2+\epsilon$$

```{r}
#   Estimating model parameters using training set 

training_ones <- matrix(1, length(x_training_set$x1),1) 


# cbind() is used to bind the matrix "ones" and other variables that are present in that model 1, together to create a new data frame, "x_model_1". 

#The resulting data frame "x_model_1" will have the same number of rows as the input variable "x" but new colum will be added,

training_model_x <- cbind(training_ones,(x_training_set[,"x1"])^4,(x_training_set[,"x2"])^2)


training_thetahat <-   solve(t(training_model_x) %*% training_model_x) %*% t(training_model_x) %*% y_training_data
```


## Model output prediction on testing data set

Following the same approach as per Task 2.2, we will calculate the model’s output/prediction on the testing data and present RSS value.

```{r}
#  y_testing_hat is being calculated as the product of the x_testing_data matrix and the estimated coefficients (training_thetahat) of the linear regression model.
# Here we get predicted values of y for the testing data.
testing_ones <- matrix(1, length(x_testing_set$x1),1)
testing_model_x <- cbind(testing_ones,(x_testing_set[,"x1"]),(x_testing_set[,"x2"]))


y_testing_hat <- testing_model_x %*% (training_thetahat)


# Calculating residual sum of squares (RSS) for the testing data
RSS_testing <- sum((y_testing_set-y_testing_hat)^2) 


# Printing RSS value of testing 

print(sprintf("RSS value is testing data %0.4f", RSS_testing))
```

## Approximate Bayesian Computation (ABC)

As we have selected model 3, now, we are going to perform rejection ABC (Approximate Bayesian Computation) for estimating the posterior distribution of two parameters - $\theta_1, \theta_2$.

### Deviate from traditional regression analysis

Calculating the likelihood and residual sum of squares (RSS) is a standard approach for regression analysis and is used to evaluate the goodness of fit of a model. However, now we will try to little deviate from traditional regression analysis - depending on likelihood function. Now, we will generate synthetic datasets based on the prior distribution of the parameters (Drechsler and Reiter, 2011), which is called Approximate Bayesian Computation (ABC).

In ABC, thus obtained synthetic datasets are compared to the observed data using a summary statistic, such as the residual sum of squares. After that we will see if the summary statistic for the synthetic data is close enough to that of the observed data, the corresponding parameter values are considered as part of the posterior distribution (Csilléry et al., 2010).

Before that we need know know about Bayesian statistic. According to (Evans, Hastings and Peacock, 2000), the use of Bayesian statistics allows for the systematic updating of beliefs based on new information citation. The fundamental theorem upon which these methods are based is known as Bayes’ theorem (Eells, 2004), and it states that given two events, A and B, the conditional probability of A given that B is true, Mathematically it is expressed as:

$$P(\theta|D)=\dfrac{P(D|\theta)P(\theta)}{P(D)}$$

$P(\theta)$ is known as the prior probability - $P(D|\theta)$ is known as the likelihood function - $P(\theta|D)$ is known as the posterior probability.

### Compute 2 parameter posterior distributions

Here, we will compute 2 parameter posterior distributions, which will be the 2 parameters with largest absolute value in our least squares estimation (Task 2.1) of the selected model 3. Then we will keep all the other parameters of our model as constant.

```{r}
# Creator vector of theta_hat of selected model 2 and sorting them to find out two largest absolute value and printing it

numbers <- c(theta_hat_3)
sorted_numbers <- sort(abs(numbers), decreasing=TRUE)
largest_two_values <- sorted_numbers[1:2]
print(largest_two_values)
```
```{r}
#Choosing parameters

theta_bias <- largest_two_values[1] 
theta_three <- largest_two_values[2]

#Constant parameter

theta_one  <- 2.715713
theta_two <--3.151350

# Initial values

arr_1 = 0
arr_2=0
f_value=0
s_value=0
theta_hat_3
```

Now we will calculate epsilon. We are using the value of epsilon as a threshold to determine whether the simulated values for the parameters are accepted or rejected. By choosing epsilon to be twice the RSS of the model, the acceptance rate will be roughly 50% (Shalizi, 2016). Then here we will be setting the number of iterations for the loop to be 100. This is not concrete, but a larger number of iterations may result in a more accurate estimate but will also increase the computational time (Bond, 2004).

### Using Uniform distribution as prior, around the estimated parameter values for 2 parameters

Based on constant and chosen parameters, we will now determine the range of the prior distribution. Then, we perform rejection ABC for those 2 parameters.

```{r}
# Calculating epsilon 

epsilon <- RSS_model_3 * 2

# Number of iteration  to determines how many times the for loop will repeat and generate new values for the parameters.

# A larger number of iterations may result in a more accurate estimate, but will also increase the computational time.
num <- 100 

##Calculating Y-hat for performing rejection ABC 

counter <- 0

# Iteration from 0 -100 and calculating the range 

for (i in 1:num) {
range1 <- runif(1,-3.65140,9.651400) 
range2 <- runif(1,-2.18139,6.18139)

# Creating new vector of  two values from range1 and range2 with the constant values theta_one,theta_three. 
New_thetahat <- matrix(c(range1,theta_one,theta_two,range2)) 

# Calculating predicted response values for the current iteration of the loop

New_Y_Hat <- x_model_3 %*% New_thetahat 

# Calculting new RSS valur
new_RSS <- sum((df$y - New_Y_Hat)^2) 



# Checking if new RSS is greater than epsilon and if the condition is true, the values of range1 and range2 are stored in arrays arr_1 and arr_2 respectively. The counter is also incremented by 1. The f_value and s_value are then defined as matrices of the arrays arr_1 and arr_2 respectively.

if (new_RSS > epsilon){
arr_1[i] <- range1 
arr_2[i] <- range2 
counter = counter+1
f_value <- matrix(arr_1)
s_value <- matrix(arr_2)

  } #closing else loop
} #closing for loop
```

Here, we checked RSS value (new_RSS) and if the RSS is smaller than the threshold value(epsilon), it indicates that the simulated data and the observed data are similar enough, and the corresponding parameter values are accepted and stored for further analysis. For which the values of range1 and range2 are stored in arrays `arr_1` and `arr_2` respectively. The counter was also incremented by 1. The `f_value` and `s_value` were then defined as matrices of the arrays `arr_1` and `arr_2` respectively.


## Plotting the joint and marginal posterior distribution for the parameters

Finally we will visualize the joint and marginal posterior distribution of the parameters using histogram.


```{r fig.cap="Frequency distribution of the f_value"}
dev.off()  
# Plotting histogram of new f_values and s_values

# Frequency distribution of the f_value
ggplot(data.frame(f_value), aes(x=f_value)) + 
  geom_histogram(color = 'black', fill = "grey") + 
  geom_rug()+ #Show the individual observations with black lines
   xlab("f_value") + ylab("Frequency ") 
```

```{r "Frequency distribution of the s_value"}
# Frequency distribution of the s_value
ggplot(data.frame(s_value), aes(x=s_value)) + 
  geom_histogram(color = 'black', fill = "grey") + 
  geom_rug()+ #Show the individual observations with black lines
  xlab("s_value") + 
  ylab("Frequency ") 
```

### Creating the Joint-pdf of the regression coefficient distribution in ABC algorithm

```{r fig.cap="Joint pdf of regression coefficients distribution"}
# Plotting the joint and marginal posterior distribution

# Create a data frame with the values of f_value and s_value and a column for "group"
df <- data.frame(f_value, s_value, legend=rep(c("f_value","s_value"), each=length(f_value)/2))

# Plot the scatter plot using and hiding legends
p <- ggplot(df, aes(x=f_value, y=s_value, color=legend)) +
  geom_point()+
  theme(legend.position="bottom")+ # show legend in bottom
  theme(legend.title = element_blank())+ # hide legend word
   #guides(color=FALSE)+ # Uncomment to hide legend
  ggtitle("Joint and Marginal Posterior Distribution")
# 
# Show the plot
print(p)
```

It is easier to view individual variable distribution and relationship in single chart.



```{r fig.cap="Joint and Marginal Posterior Distribution"}
# Plotting the joint and marginal posterior distribution

# Create a data frame with the values of f_value and s_value and a column for "group"
df <- data.frame(f_value, s_value, legend=rep(c("f_value","s_value"), each=length(f_value)/2))

# Plot the scatter plot using and hiding legends
p <- ggplot(df, aes(x=f_value, y=s_value, color=legend)) +
  geom_point()+
  theme(legend.position="bottom")+
  theme(legend.title = element_blank())
   #guides(color=FALSE)+ # Uncomment to hide legend
  # 
# # Show the plot
# print(p)


# Marginal histograms by group
ggMarginal(p, type = "histogram",
          xparams = list(fill = 1),
           yparams = list(fill = 1)) 

```

```{r, fig.cap="Joint distribution as a contour plot"}
ggplot(df, aes(x=f_value, y=s_value) ) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon")+
  theme_bw()
```

By visualizing the `f_value` and `s_value` we get a visual representation of the parameters that gave a good fit between the simulated and observed data. Also, the histograms on the side provide a visual representation of the distribution of the individual variables, rather than their joint distribution.

## Conclusion and opportunities for improvement

Based on a thorough analysis, we are able to conclude that our objectives have been achieved to a satisfactory degree. Despite this, there are still opportunities for improvement, including:

Insufficient data: Polynomial regression requires a sufficient amount of data to accurately model the relationship and we are provided with just 2400 number of rows. Having more numbers of data would be best to explore underlying patterns in the EEG signals.

Overfitting: According to (Xu and Goodacre, 2018), cross-validation is considered to be better than a 70/30% split of the data as 70/30% split can result in overfitting, where the model is trained too well on the training data, but performs poorly on the test data. On top of this, cross validation also helps in reducing the bias in the model evaluation by using multiple subsets of the data instead of a single fixed subset.

Non-linearity: The relationship between input and output signals in real-life EEG is non-linear in nature. It may not be possible to model these complex relationships accurately using polynomial regression, which is used as linear combination of polynomial functions.

And by addressing these issues we can improve our EEG signal modeling.


# References {-}

